{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import numpy as np  \n","import pandas as pd  \n","from datetime import datetime\n","from scipy.stats import skew  # for some statistics\n","from scipy.special import boxcox1p\n","from scipy.stats import boxcox_normmax\n","from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge,Lasso,ElasticNet\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor,RandomForestClassifier\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.svm import SVR, LinearSVC\n","from sklearn.pipeline import make_pipeline,Pipeline\n","from sklearn.preprocessing import RobustScaler, LabelEncoder\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","\n","from mlxtend.regressor import StackingCVRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","import lightgbm as lgb\n","import os\n","import re\n","import shap\n","import pandas_profiling as ppf\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import missingno as msno\n","\n","import re\n","from collections import Counter\n","from operator import itemgetter\n","import time\n","from itertools import product\n","import datetime as dt\n","import calendar\n","import gc\n","\n","RANDOM_SEED = 42"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["PATH = '../input/competitive-data-science-predict-future-sales'\n","items = pd.read_csv(PATH + '/items.csv')\n","shops = pd.read_csv(PATH + '/shops.csv')\n","cats = pd.read_csv(PATH + '/item_categories.csv')\n","train = pd.read_csv(PATH + '/sales_train.csv')\n","# set index to ID to avoid droping it later\n","test  = pd.read_csv(PATH + '/test.csv').set_index('ID')"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train.date = pd.to_datetime(train.date.index)\n","print(train.head(2))\n","(test.head(2))"],"execution_count":6,"outputs":[{"output_type":"stream","text":"         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n0  02.01.2013               0       59    22154       999.0           1.0\n1  03.01.2013               0       25     2552       899.0           1.0\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"    shop_id  item_id\nID                  \n0         5     5037\n1         5     5320","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shop_id</th>\n      <th>item_id</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>5037</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>5320</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def summary_stats_table(data):\n","    '''\n","    function to describe the data\n","    '''\n","    # count of nulls\n","    missing_counts = pd.DataFrame(data.isnull().sum())\n","    missing_counts.columns = ['count_null']\n","\n","    # numeric column stats\n","    num_stats = data.select_dtypes(include=['int64','float64']).describe().loc[['count','min','max']].transpose()\n","    num_stats['dtype'] = data.select_dtypes(include=['int64','float64']).dtypes.tolist()\n","\n","    # non-numeric value stats\n","    non_num_stats = data.select_dtypes(exclude=['int64','float64']).describe().transpose()\n","    non_num_stats['dtype'] = data.select_dtypes(exclude=['int64','float64']).dtypes.tolist()\n","    non_num_stats = non_num_stats.rename(columns={\"first\": \"min\", \"last\": \"max\"})\n","\n","    # merge all \n","    stats_merge = pd.concat([num_stats, non_num_stats], axis=0, join='outer', ignore_index=False, keys=None,\n","              levels=None, names=None, verify_integrity=False, copy=True).fillna(\"\").sort_values('dtype')\n","\n","    column_order = ['dtype', 'count', 'count_null','unique','min','max','top','freq']\n","    summary_stats = pd.merge(stats_merge, missing_counts, left_index=True, right_index=True)[column_order]\n","    return(summary_stats)"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Check data"]},{"metadata":{},"cell_type":"markdown","source":["## File\n","* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n","* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n","* sample_submission.csv - a sample submission file in the correct format.\n","* items.csv - supplemental information about the items/products.\n","* item_categories.csv  - supplemental information about the items categories.\n","* shops.csv- supplemental information about the shops.\n","\n","## Columns\n","\n","* ID - an Id that represents a (Shop, Item) tuple within the test set\n","* shop_id - unique identifier of a shop\n","* item_id - unique identifier of a product\n","* item_category_id - unique identifier of item category\n","* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n","* item_price - current price of an item\n","* date - date in format dd/mm/yyyy\n","* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","* item_name - name of item\n","* shop_name - name of shop\n","* item_category_name - name of item category"]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(items.head())\n","print(shops.head())\n","print(cats.head())\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":"                                           item_name  item_id  \\\n0          ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0   \n1  !ABBYY FineReader 12 Professional Edition Full...        1   \n2      ***В ЛУЧАХ СЛАВЫ   (UNV)                    D        2   \n3    ***ГОЛУБАЯ ВОЛНА  (Univ)                      D        3   \n4        ***КОРОБКА (СТЕКЛО)                       D        4   \n\n   item_category_id  \n0                40  \n1                76  \n2                40  \n3                40  \n4                40  \n                        shop_name  shop_id\n0   !Якутск Орджоникидзе, 56 фран        0\n1   !Якутск ТЦ \"Центральный\" фран        1\n2                Адыгея ТЦ \"Мега\"        2\n3  Балашиха ТРК \"Октябрь-Киномир\"        3\n4        Волжский ТЦ \"Волга Молл\"        4\n        item_category_name  item_category_id\n0  PC - Гарнитуры/Наушники                 0\n1         Аксессуары - PS2                 1\n2         Аксессуары - PS3                 2\n3         Аксессуары - PS4                 3\n4         Аксессуары - PSP                 4\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["summary_stats_table(train)"],"execution_count":9,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n","name":"stderr"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                  dtype      count  count_null unique min     max         top  \\\ndate_block_num    int64  2935849.0           0          0      33               \nshop_id           int64  2935849.0           0          0      59               \nitem_id           int64  2935849.0           0          0   22169               \nitem_price      float64  2935849.0           0         -1  307980               \nitem_cnt_day    float64  2935849.0           0        -22    2169               \ndate             object  2935849.0           0   1034              28.12.2013   \n\n                freq  \ndate_block_num        \nshop_id               \nitem_id               \nitem_price            \nitem_cnt_day          \ndate            9434  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dtype</th>\n      <th>count</th>\n      <th>count_null</th>\n      <th>unique</th>\n      <th>min</th>\n      <th>max</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>date_block_num</th>\n      <td>int64</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td></td>\n      <td>0</td>\n      <td>33</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>shop_id</th>\n      <td>int64</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td></td>\n      <td>0</td>\n      <td>59</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <td>int64</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td></td>\n      <td>0</td>\n      <td>22169</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>item_price</th>\n      <td>float64</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td></td>\n      <td>-1</td>\n      <td>307980</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>item_cnt_day</th>\n      <td>float64</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td></td>\n      <td>-22</td>\n      <td>2169</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>date</th>\n      <td>object</td>\n      <td>2935849.0</td>\n      <td>0</td>\n      <td>1034</td>\n      <td></td>\n      <td></td>\n      <td>28.12.2013</td>\n      <td>9434</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":["# Data clearn"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Drop item_price and item_cnt_day too high\n","train = train[train.item_price<100000] # drop 1\n","train = train[train.item_cnt_day<1000] # drop 2\n","\n","## Drop negative price \n","train = train[train.item_price > 0].reset_index(drop=True) # drop 1\n","\n","## The item has been returned \n","train.loc[train.item_cnt_day < 0, 'item_cnt_day'] = 0\n","\n","## Fuse some shop in train and test set\n","# Якутск Орджоникидзе, 56\n","train.loc[train.shop_id == 0, 'shop_id'] = 57\n","test.loc[test.shop_id == 0, 'shop_id'] = 57\n","# Якутск ТЦ \"Центральный\"\n","train.loc[train.shop_id == 1, 'shop_id'] = 58\n","test.loc[test.shop_id == 1, 'shop_id'] = 58\n","# Жуковский ул. Чкалова 39м²\n","train.loc[train.shop_id == 11, 'shop_id'] = 10\n","test.loc[test.shop_id == 11, 'shop_id'] = 10\n","# РостовНаДону ТРК \"Мегацентр Горизонт\" Островной\n","train.loc[train.shop_id == 40, 'shop_id'] = 39\n","test.loc[test.shop_id == 40, 'shop_id'] = 39\n","\n"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Categorize shops info\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Define shop city and category\n","shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n","\n","shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n","shops['category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\n","\n","shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n","\n","## Shop category\n","category = ['ТЦ', 'ТРК', 'ТРЦ', 'ТК']\n","shops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\n","shops.groupby(['category']).sum()\n","\n","## Shop city\n","shops['shop_city'] = shops.city\n","shops['shop_category'] = shops.category\n","\n","shops['shop_city'] = LabelEncoder().fit_transform(shops['shop_city'])\n","shops['shop_category'] = LabelEncoder().fit_transform(shops['shop_category'])\n","\n","shops = shops[['shop_id','shop_city', 'shop_category']]\n","shops.head()\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"   shop_id  shop_city  shop_category\n0        0         29              0\n1        1         29              4\n2        2          0              4\n3        3          1              2\n4        4          2              4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shop_id</th>\n      <th>shop_city</th>\n      <th>shop_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>29</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":["# cats info"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Fuse some item_category_name\n","cats['type_code'] = cats.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\n","cats.loc[(cats.type_code == 'Игровые') | (cats.type_code == 'Аксессуары'), 'type_code'] = 'Игры'\n","cats.loc[cats.type_code == 'PC', 'type_code'] = 'Музыка'\n","\n","## Labelencoder item_category_name (main category) to type_code\n","category = ['Игры', 'Карты', 'Кино', 'Книги','Музыка', 'Подарки', 'Программы', 'Служебные', 'Чистые']\n","cats['type_code'] = cats.type_code.apply(lambda x: x if (x in category) else 'etc')\n","cats['type_code'] = LabelEncoder().fit_transform(cats['type_code'])\n","\n","## Labelencoder item_category_name (sub category) to subtype_code\n","cats['split'] = cats.item_category_name.apply(lambda x: x.split('-'))\n","cats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n","cats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n","\n","cats = cats[['item_category_id','type_code', 'subtype_code']]\n"],"execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# items info"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Extract info in [] and ()\n","items['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\n","items['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n","\n","## Replace symbol by ',])'... by ' ' \n","items['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\n","items['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\n","items = items.fillna('0')\n","\n","## Test\n","result_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\n","result_1 = sorted(result_1, key=itemgetter(1))\n","result_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\n","result_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n","\n","result_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\n","result_2 = sorted(result_2, key=itemgetter(1))\n","result_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\n","result_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n","\n","result = pd.concat([result_1, result_2])\n","result = result.drop_duplicates(subset=['feature']).reset_index(drop=True)\n","\n","print('Most common aditional features:', result)\n","\n","\n","## Fuse some types\n","items['type'] = items.name_2.apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0])\n","items.loc[(items.type == 'x360') | (items.type == 'xbox360'), 'type'] = 'xbox 360'\n","items.loc[items.type == '', 'type'] = 'mac'\n","items.type = items.type.apply(lambda x: x.replace(' ',''))\n","items.loc[(items.type == 'pc') | (items.type == 'pс') | (items.type == 'рс'), 'type'] = 'pc'\n","items.loc[(items.type == 'рs3'), 'type'] = 'ps3'\n","\n","## Find low frequence item type\n","# group_count = items[['item_id', 'type']].groupby('type').count()\n","# drop_list = group_count.loc[group_count.item_id < 10].index\n","group_sum = items.groupby('type').sum()\n","drop_list = group_sum.loc[group_sum.item_category_id < 200].index\n","\n","print('drop list:', drop_list)\n","\n","## Replece low frequence item type by etc in name_2\n","items.name_2 = items.type.apply(lambda x: 'etc' if x in drop_list else x)\n","items = items.drop(['type'], axis=1)\n","print(items.groupby('name_2').count()[['item_id']])\n","\n","## Labelencoder name_2, name_3\n","items['name_2'] = LabelEncoder().fit_transform(items['name_2']).astype(np.int8)\n","items['name_3'] = LabelEncoder().fit_transform(items['name_3']).astype(np.int16)\n","items.drop(['item_name', 'name_1'], axis=1, inplace=True)"],"execution_count":15,"outputs":[{"output_type":"stream","text":"Most common aditional features:          feature  count\n0   документация    284\n1     английская    340\n2        русские    399\n3       субтитры    400\n4            360    465\n5          jewel    552\n6           xbox    589\n7            ps3    611\n8        русская   1428\n9       цифровая   1995\n10            pc   2585\n11        версия   3427\n12           box    246\n13            3d    409\n14           dvd    503\n15      digipack    541\n16          фирм    757\n17           mp3    854\n18            cd    871\n19        регион   1849\n20            bd   2320\ndrop list: Index(['5c5', '5c7', '5f4', '6dv', '6jv', '6l6', 'android', 'hm3', 'j72',\n       'kf6', 'kf7', 'kg4', 'p', 'ps2', 's3v', 's4v', 'англ', 'русская',\n       'только', 'цифров'],\n      dtype='object', name='type')\n          item_id\nname_2           \n0           17661\netc            35\nmac            42\npc           2642\nps             79\nps3           611\nps4           174\npsp           115\nxbox360       466\nxboxone       123\nцифровая      222\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Month sales treatment\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Prepare matrix for (shop_id，item_id) couple\n","ts = time.time()\n","matrix = []\n","cols = ['date_block_num','shop_id','item_id']\n","for i in range(34):\n","    sales = train[train.date_block_num==i]\n","    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n","    \n","matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n","matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n","matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n","matrix['item_id'] = matrix['item_id'].astype(np.int16)\n","matrix.sort_values(cols,inplace=True)\n","print('Use time:', time.time() - ts)\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":"Use time: 18.469717741012573\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["## revenue from one item on one day's sale\n","train['revenue'] = train['item_price'] *  train['item_cnt_day']\n","\n","## Fill known month sale data in the matrix (all (shop_id，item_id) couple)\n","ts = time.time()\n","group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n","\n","group.columns = ['item_cnt_month']\n","group.reset_index(inplace=True)\n","matrix = pd.merge(matrix, group, on=cols, how='left')\n","matrix['item_cnt_month'] = (matrix['item_cnt_month']\n","                                .fillna(0)#.astype(np.float16))\n","                                .clip(0,20) # NB clip target here\n","                                .astype(np.float16))\n","print('Use time:', time.time() - ts)\n","\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":"Use time: 5.649743318557739\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Test set preparation"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Add test 34 month to matrix\n","test['date_block_num'] = 34\n","test['date_block_num'] = test['date_block_num'].astype(np.int8)\n","test['shop_id'] = test['shop_id'].astype(np.int8)\n","test['item_id'] = test['item_id'].astype(np.int16)\n","\n","ts = time.time()\n","matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\n","matrix.fillna(0, inplace=True) # 34 month\n","print('Use time:', time.time() - ts)\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":"Use time: 0.0730586051940918\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Shops/Items/Cats features"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Attach shop, item, item_category information to the matrix\n","\n","ts = time.time()\n","matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n","matrix = pd.merge(matrix, items, on=['item_id'], how='left')\n","matrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\n","matrix['shop_city'] = matrix['shop_city'].astype(np.int8)\n","matrix['shop_category'] = matrix['shop_category'].astype(np.int8)\n","matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n","matrix['type_code'] = matrix['type_code'].astype(np.int8)\n","matrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n","\n","print('Use time:', time.time() - ts)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":"Use time: 4.406449556350708\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Target lags"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def lag_feature(df, lags, col):\n","    tmp = df[['date_block_num','shop_id','item_id',col]]\n","    for i in lags:\n","        shifted = tmp.copy()\n","        shifted.columns = ['date_block_num','shop_id','item_id', col+'-lag'+str(i)]\n","        shifted['date_block_num'] += i\n","        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n","    return df"],"execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Lag 1,2,3 month\n","ts = time.time()\n","matrix = lag_feature(matrix, [1,2,3], 'item_cnt_month')\n","print('Use time:', time.time() - ts)\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":"Use time: 21.792207717895508\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Mean encoded features"]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":["## Simplyfy feauture name dictionaries:\n","dict_simple = {'date_block_num': 'date', 'item_id': 'item', 'shop_id': 'shop', \n","               'item_category_id': 'itemcate', 'item_price':'price', \n","               \n","               'item_cnt_month': 'cnt', }\n","\n","## Functions for groupby and aggregate\n","def sum_names(name_list):\n","    names = ''\n","    for x in name_list:\n","        names += x+'+'\n","    return names\n","    \n","def group_agg(matrix, groupby_feats, transform_feat, aggtype='mean'):\n","    group = matrix.groupby(groupby_feats).agg({transform_feat: [aggtype]})\n","    groupby_feats_simple = [dict_simple[x] if x in dict_simple.keys() else x\n","                            for x in groupby_feats]\n","    transform_feat_simple = dict_simple[transform_feat] \\\n","                            if transform_feat in dict_simple.keys() else transform_feat\n","    group_name = f'{sum_names(groupby_feats_simple)[:-1]}-{aggtype.upper()}-{transform_feat_simple}'\n","    group.columns = [ group_name ]\n","    group.reset_index(inplace=True)\n","    return group, group_name\n","    \n","def add_groupmean_lag(matrix, groupby_feats, transform_feat, lags):\n","    group, group_name = group_agg(matrix, groupby_feats, transform_feat)\n","    \n","    matrix = pd.merge(matrix, group, on=groupby_feats, how='left')\n","    matrix[group_name] = matrix[group_name].astype(np.float16)\n","    if lags != []:\n","        matrix = lag_feature(matrix, lags, group_name)\n","        matrix.drop([group_name], axis=1, inplace=True)\n","    return matrix\n","    \n"],"execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ts = time.time()\n","\n","## Compute mean item_cnt_month (cnt) \n","transform_feat = 'item_cnt_month'\n","\n","\n","groupby_feats = ['date_block_num']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'item_id']\n","lags = [1,2,3]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'shop_id']\n","lags = [1,2,3]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'item_category_id']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'shop_id', 'item_category_id']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'shop_id', 'item_id']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'shop_id', 'subtype_code']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'shop_city']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","groupby_feats = ['date_block_num', 'item_id', 'shop_city']\n","lags = [1]\n","matrix = add_groupmean_lag(matrix, groupby_feats, transform_feat, lags)\n","\n","print('Use time:', time.time() - ts)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":"Use time: 167.35561776161194\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Trend Features\n","## Price trend"]},{"metadata":{"trusted":true},"cell_type":"code","source":["ts = time.time()\n","\n","fetures_to_drop = []\n","\n","## caculate mean price for every item\n","transform_feat = 'item_price'\n","groupby_feats = ['item_id']\n","group, mean_price_col = group_agg(train, groupby_feats, \n","                                  transform_feat, aggtype='mean')\n","matrix = pd.merge(matrix, group, on=groupby_feats, how='left')\n","matrix[mean_price_col] = matrix[mean_price_col].astype(np.float16)\n","\n","\n","## caculate mean date (monthly) price for every item\n","transform_feat = 'item_price'\n","groupby_feats = ['date_block_num','item_id']\n","group, mean_monthlyprice_col = group_agg(train, groupby_feats, \n","                                         transform_feat, aggtype='mean')\n","matrix = pd.merge(matrix, group, on=groupby_feats, how='left')\n","matrix[mean_monthlyprice_col] = matrix[mean_monthlyprice_col].astype(np.float16)\n","\n","\n","## create time lags for date date (monthly) price\n","lags = [1,2,3]\n","matrix = lag_feature(matrix, lags, mean_monthlyprice_col)\n","\n","## delta between date (monthly) mean and mean price for every lag\n","for i in lags:\n","    matrix['delta_price-lag'+str(i)] = \\\n","    (matrix[f'{mean_monthlyprice_col}-lag'+str(i)] - matrix[mean_price_col])\\\n","    / matrix[mean_price_col]\n","\n","matrix['delta_price-lag']=0\n","bool_loc = np.ones(len(matrix))==1\n","for i in lags:   \n","    matrix.loc[bool_loc, 'delta_price-lag'] = matrix.loc[bool_loc,'delta_price-lag'+str(i)]\n","    bool_loc &= matrix['delta_price-lag'+str(i)]==0\n","matrix['delta_price-lag'] = matrix['delta_price-lag'].astype(np.float16)\n","matrix['delta_price-lag'].fillna(0, inplace=True)\n","\n","## Only keep 'delta_price_lag' feature\n","fetures_to_drop.append(mean_price_col)\n","fetures_to_drop.append(mean_monthlyprice_col)\n","for i in lags:\n","    fetures_to_drop += [f'{mean_monthlyprice_col}-lag'+str(i)]\n","    fetures_to_drop += ['delta_price-lag'+str(i)]\n","\n","matrix.drop(fetures_to_drop, axis=1, inplace=True)\n","print('Use time:', time.time() - ts)\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":"Use time: 34.32036828994751\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## Revenue trend"]},{"metadata":{"trusted":true},"cell_type":"code","source":["ts = time.time()\n","\n","## Monthly shop revenue sum\n","group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n","group.columns = ['date_shop_revenue']\n","group.reset_index(inplace=True)\n","\n","matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n","matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n","\n","\n","## Shop mean revenue from Monthly shop revenue sum\n","group = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n","group.columns = ['shop_avg_revenue']\n","group.reset_index(inplace=True)\n","\n","matrix = pd.merge(matrix, group, on=['shop_id'], how='left')\n","matrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n","\n","## delta between date (monthly) revenue and mean revenue\n","matrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n","matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n","\n","## revenue lags\n","matrix = lag_feature(matrix, [1], 'delta_revenue')\n","\n","## Only keep lag 'delta_revenue' feature\n","matrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n","\n","print('Use time:', time.time() - ts)\n","\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":"Use time: 16.25323748588562\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Date info"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Date information\n","total_block_num = 35\n","date_block_num = np.arange(total_block_num)\n","date_block = [pd.Timestamp(2013, 1, 1)+pd.DateOffset(months=x) for x in date_block_num]\n","\n","\n","df_date = pd.DataFrame(date_block_num, columns=['date_block_num'])\n","df_date['date_block'] = date_block\n","df_date['year'] = df_date['date_block'].dt.year\n","df_date['month'] = df_date['date_block'].dt.month\n","\n","for i in range(len(df_date)):\n","    day_to_count = 0\n","    calendar_matrix = calendar.monthcalendar(df_date['year'].iloc[i],df_date['month'].iloc[i])\n","    for j in range(7): # 7 days a week\n","        num_days = sum(1 for x in calendar_matrix if x[j] != 0)\n","        df_date.loc[i, f'week{j}'] = num_days\n","df_date = df_date[['date_block_num', 'year','month','week0','week1',\n","                   'week2','week3','week4','week5','week6']]  \n","df_date['days'] = df_date[['week0','week1','week2','week3','week4','week5','week6']].sum(axis=1)\n","df_date['year'] = df_date['year']-2012\n","df_date = df_date.astype(np.int8)\n","\n","matrix = pd.merge(matrix, df_date, on=['date_block_num'], how='left')\n"],"execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## The first month when one item is on sale\n","matrix['item_shop_first_sale'] = \\\n","matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n","\n","matrix['item_first_sale'] = \\\n","matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n","\n"],"execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Drop 3 first date (because of lag)\n","matrix = matrix[matrix.date_block_num > 3]\n","\n"],"execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Fill NA\n","\n","def fill_na(df):\n","    for col in df.columns:\n","        if ('-lag' in col) & (df[col].isnull().any()):\n","            print(col)\n","            if ('cnt' in col):\n","                df[col].fillna(0, inplace=True)         \n","    return df\n","\n","matrix = fill_na(matrix)"],"execution_count":29,"outputs":[{"output_type":"stream","text":"item_cnt_month-lag1\nitem_cnt_month-lag2\nitem_cnt_month-lag3\ndate-MEAN-cnt-lag1\ndate+item-MEAN-cnt-lag1\ndate+item-MEAN-cnt-lag2\ndate+item-MEAN-cnt-lag3\ndate+shop-MEAN-cnt-lag1\ndate+shop-MEAN-cnt-lag2\ndate+shop-MEAN-cnt-lag3\ndate+itemcate-MEAN-cnt-lag1\ndate+shop+itemcate-MEAN-cnt-lag1\ndate+shop+item-MEAN-cnt-lag1\ndate+shop+subtype_code-MEAN-cnt-lag1\ndate+shop_city-MEAN-cnt-lag1\ndate+item+shop_city-MEAN-cnt-lag1\ndelta_revenue-lag1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["matrix.isna().sum()"],"execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"date_block_num                                0\nshop_id                                       0\nitem_id                                       0\nitem_cnt_month                                0\nshop_city                                     0\nshop_category                                 0\nitem_category_id                              0\nname_2                                        0\nname_3                                        0\ntype_code                                     0\nsubtype_code                                  0\nitem_cnt_month-lag1                           0\nitem_cnt_month-lag2                           0\nitem_cnt_month-lag3                           0\ndate-MEAN-cnt-lag1                            0\ndate+item-MEAN-cnt-lag1                       0\ndate+item-MEAN-cnt-lag2                       0\ndate+item-MEAN-cnt-lag3                       0\ndate+shop-MEAN-cnt-lag1                       0\ndate+shop-MEAN-cnt-lag2                       0\ndate+shop-MEAN-cnt-lag3                       0\ndate+itemcate-MEAN-cnt-lag1                   0\ndate+shop+itemcate-MEAN-cnt-lag1              0\ndate+shop+item-MEAN-cnt-lag1                  0\ndate+shop+subtype_code-MEAN-cnt-lag1          0\ndate+shop_city-MEAN-cnt-lag1                  0\ndate+item+shop_city-MEAN-cnt-lag1             0\ndelta_price-lag                               0\ndelta_revenue-lag1                      1937539\nyear                                          0\nmonth                                         0\nweek0                                         0\nweek1                                         0\nweek2                                         0\nweek3                                         0\nweek4                                         0\nweek5                                         0\nweek6                                         0\ndays                                          0\nitem_shop_first_sale                          0\nitem_first_sale                               0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pickle\n","import gc\n","\n","del group\n","del items\n","del shops\n","del cats\n","del train\n","# leave test for submission\n","gc.collect();\n","\n","matrix.to_pickle('../working/data.pkl')\n","\n","del matrix\n","gc.collect();"],"execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Train "]},{"metadata":{"trusted":true},"cell_type":"code","source":["data = pd.read_pickle('../working/data.pkl')\n","# data = matrix\n","# del matrix\n","\n","test  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\n","\n","print(len(data.columns))\n","data.columns"],"execution_count":32,"outputs":[{"output_type":"stream","text":"41\n","name":"stdout"},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"Index(['date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'shop_city',\n       'shop_category', 'item_category_id', 'name_2', 'name_3', 'type_code',\n       'subtype_code', 'item_cnt_month-lag1', 'item_cnt_month-lag2',\n       'item_cnt_month-lag3', 'date-MEAN-cnt-lag1', 'date+item-MEAN-cnt-lag1',\n       'date+item-MEAN-cnt-lag2', 'date+item-MEAN-cnt-lag3',\n       'date+shop-MEAN-cnt-lag1', 'date+shop-MEAN-cnt-lag2',\n       'date+shop-MEAN-cnt-lag3', 'date+itemcate-MEAN-cnt-lag1',\n       'date+shop+itemcate-MEAN-cnt-lag1', 'date+shop+item-MEAN-cnt-lag1',\n       'date+shop+subtype_code-MEAN-cnt-lag1', 'date+shop_city-MEAN-cnt-lag1',\n       'date+item+shop_city-MEAN-cnt-lag1', 'delta_price-lag',\n       'delta_revenue-lag1', 'year', 'month', 'week0', 'week1', 'week2',\n       'week3', 'week4', 'week5', 'week6', 'days', 'item_shop_first_sale',\n       'item_first_sale'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n","Y_train = data[data.date_block_num < 33]['item_cnt_month']\n","\n","X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n","Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n","X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n","\n","del data\n","gc.collect();"],"execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["X_train.info()"],"execution_count":34,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 9106440 entries, 1497465 to 10603904\nData columns (total 40 columns):\ndate_block_num                          int8\nshop_id                                 int8\nitem_id                                 int16\nshop_city                               int8\nshop_category                           int8\nitem_category_id                        int8\nname_2                                  int8\nname_3                                  int16\ntype_code                               int8\nsubtype_code                            int8\nitem_cnt_month-lag1                     float16\nitem_cnt_month-lag2                     float16\nitem_cnt_month-lag3                     float16\ndate-MEAN-cnt-lag1                      float16\ndate+item-MEAN-cnt-lag1                 float16\ndate+item-MEAN-cnt-lag2                 float16\ndate+item-MEAN-cnt-lag3                 float16\ndate+shop-MEAN-cnt-lag1                 float16\ndate+shop-MEAN-cnt-lag2                 float16\ndate+shop-MEAN-cnt-lag3                 float16\ndate+itemcate-MEAN-cnt-lag1             float16\ndate+shop+itemcate-MEAN-cnt-lag1        float16\ndate+shop+item-MEAN-cnt-lag1            float16\ndate+shop+subtype_code-MEAN-cnt-lag1    float16\ndate+shop_city-MEAN-cnt-lag1            float16\ndate+item+shop_city-MEAN-cnt-lag1       float16\ndelta_price-lag                         float16\ndelta_revenue-lag1                      float16\nyear                                    int8\nmonth                                   int8\nweek0                                   int8\nweek1                                   int8\nweek2                                   int8\nweek3                                   int8\nweek4                                   int8\nweek5                                   int8\nweek6                                   int8\ndays                                    int8\nitem_shop_first_sale                    int8\nitem_first_sale                         int8\ndtypes: float16(18), int16(2), int8(20)\nmemory usage: 590.6 MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["lgb_train = lgb.Dataset(X_train, Y_train)\n","lgb_eval = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)\n","\n","del X_train\n","gc.collect();"],"execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def rmsle(y, y_pred):\n","    return np.sqrt(mean_squared_error(y, y_pred))\n","\n","# params = {'num_leaves': 512, 'max_depth': 25, 'max_bin': 128, 'n_estimators': 3000, \n","#           'bagging_freq': 7, 'bagging_fraction': 0.6, \n","#           'feature_fraction': 0.2, 'min_data_in_leaf': 78, \n","#           'learning_rate': 0.1, 'num_threads': 6, \n","#           'min_sum_hessian_in_leaf': 6,\n","\n","#           'random_state' : RANDOM_SEED,\n","#           'verbosity' : 1,\n","#           'bagging_seed' : RANDOM_SEED,\n","#           'boost_from_average' : 'true',\n","#           'boost' : 'gbdt',\n","#           'metric' : 'rmse',\n","# }\n","\n","params = {'num_leaves': 2000, 'max_depth': 19, 'max_bin': 107, 'n_estimators': 3747,\n","          'bagging_freq': 1, 'bagging_fraction': 0.8, \n","          'feature_fraction': 0.5, 'min_data_in_leaf': 88, \n","          'learning_rate': 0.015, 'num_threads': 3, \n","          'min_sum_hessian_in_leaf': 6,\n","         \n","          'random_state' : RANDOM_SEED,\n","          'verbosity' : 1,\n","          'bagging_seed' : RANDOM_SEED,\n","          'boost_from_average' : 'true',\n","          'boost' : 'gbdt',\n","          'metric' : 'rmse',}\n","\n","model = lgb.train(params,\n","                lgb_train,\n","                num_boost_round=20,\n","                valid_sets=[lgb_train,lgb_eval],\n","                early_stopping_rounds=20,\n","                verbose_eval=1,\n","                )\n","y_pred = model.predict(X_valid)\n","rmsle(Y_valid, y_pred)"],"execution_count":36,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","name":"stderr"},{"output_type":"stream","text":"[1]\ttraining's rmse: 1.21239\tvalid_1's rmse: 1.13077\nTraining until validation scores don't improve for 20 rounds\n[2]\ttraining's rmse: 1.20299\tvalid_1's rmse: 1.12437\n[3]\ttraining's rmse: 1.19389\tvalid_1's rmse: 1.11847\n[4]\ttraining's rmse: 1.1851\tvalid_1's rmse: 1.1118\n[5]\ttraining's rmse: 1.17653\tvalid_1's rmse: 1.10536\n[6]\ttraining's rmse: 1.16788\tvalid_1's rmse: 1.09874\n[7]\ttraining's rmse: 1.15953\tvalid_1's rmse: 1.09317\n[8]\ttraining's rmse: 1.15118\tvalid_1's rmse: 1.08703\n[9]\ttraining's rmse: 1.14345\tvalid_1's rmse: 1.08148\n[10]\ttraining's rmse: 1.13668\tvalid_1's rmse: 1.07672\n[11]\ttraining's rmse: 1.12941\tvalid_1's rmse: 1.07229\n[12]\ttraining's rmse: 1.12148\tvalid_1's rmse: 1.06689\n[13]\ttraining's rmse: 1.11388\tvalid_1's rmse: 1.06166\n[14]\ttraining's rmse: 1.10661\tvalid_1's rmse: 1.05627\n[15]\ttraining's rmse: 1.0992\tvalid_1's rmse: 1.05164\n[16]\ttraining's rmse: 1.09214\tvalid_1's rmse: 1.04607\n[17]\ttraining's rmse: 1.08529\tvalid_1's rmse: 1.04088\n[18]\ttraining's rmse: 1.0783\tvalid_1's rmse: 1.03607\n[19]\ttraining's rmse: 1.07269\tvalid_1's rmse: 1.03203\n[20]\ttraining's rmse: 1.06632\tvalid_1's rmse: 1.02743\n[21]\ttraining's rmse: 1.06032\tvalid_1's rmse: 1.02425\n[22]\ttraining's rmse: 1.05427\tvalid_1's rmse: 1.02047\n[23]\ttraining's rmse: 1.04907\tvalid_1's rmse: 1.01683\n[24]\ttraining's rmse: 1.04428\tvalid_1's rmse: 1.01405\n[25]\ttraining's rmse: 1.03833\tvalid_1's rmse: 1.01059\n[26]\ttraining's rmse: 1.03274\tvalid_1's rmse: 1.00664\n[27]\ttraining's rmse: 1.02705\tvalid_1's rmse: 1.00342\n[28]\ttraining's rmse: 1.02227\tvalid_1's rmse: 1.00033\n[29]\ttraining's rmse: 1.01703\tvalid_1's rmse: 0.997083\n[30]\ttraining's rmse: 1.01182\tvalid_1's rmse: 0.993944\n[31]\ttraining's rmse: 1.00688\tvalid_1's rmse: 0.990668\n[32]\ttraining's rmse: 1.00236\tvalid_1's rmse: 0.987326\n[33]\ttraining's rmse: 0.997818\tvalid_1's rmse: 0.984418\n[34]\ttraining's rmse: 0.993455\tvalid_1's rmse: 0.981281\n[35]\ttraining's rmse: 0.988939\tvalid_1's rmse: 0.978633\n[36]\ttraining's rmse: 0.984361\tvalid_1's rmse: 0.975954\n[37]\ttraining's rmse: 0.980046\tvalid_1's rmse: 0.973129\n[38]\ttraining's rmse: 0.975572\tvalid_1's rmse: 0.971169\n[39]\ttraining's rmse: 0.971263\tvalid_1's rmse: 0.968404\n[40]\ttraining's rmse: 0.967133\tvalid_1's rmse: 0.965921\n[41]\ttraining's rmse: 0.963464\tvalid_1's rmse: 0.963518\n[42]\ttraining's rmse: 0.95951\tvalid_1's rmse: 0.961429\n[43]\ttraining's rmse: 0.955818\tvalid_1's rmse: 0.958936\n[44]\ttraining's rmse: 0.951968\tvalid_1's rmse: 0.956589\n[45]\ttraining's rmse: 0.948174\tvalid_1's rmse: 0.954504\n[46]\ttraining's rmse: 0.944419\tvalid_1's rmse: 0.952353\n[47]\ttraining's rmse: 0.940522\tvalid_1's rmse: 0.950267\n[48]\ttraining's rmse: 0.936999\tvalid_1's rmse: 0.948417\n[49]\ttraining's rmse: 0.933505\tvalid_1's rmse: 0.946284\n[50]\ttraining's rmse: 0.930212\tvalid_1's rmse: 0.944557\n[51]\ttraining's rmse: 0.927031\tvalid_1's rmse: 0.942555\n[52]\ttraining's rmse: 0.92405\tvalid_1's rmse: 0.940657\n[53]\ttraining's rmse: 0.920615\tvalid_1's rmse: 0.938887\n[54]\ttraining's rmse: 0.917621\tvalid_1's rmse: 0.937462\n[55]\ttraining's rmse: 0.914696\tvalid_1's rmse: 0.93619\n[56]\ttraining's rmse: 0.91166\tvalid_1's rmse: 0.934476\n[57]\ttraining's rmse: 0.908901\tvalid_1's rmse: 0.933249\n[58]\ttraining's rmse: 0.906334\tvalid_1's rmse: 0.931885\n[59]\ttraining's rmse: 0.903972\tvalid_1's rmse: 0.930755\n[60]\ttraining's rmse: 0.90138\tvalid_1's rmse: 0.929904\n[61]\ttraining's rmse: 0.898894\tvalid_1's rmse: 0.928647\n[62]\ttraining's rmse: 0.896442\tvalid_1's rmse: 0.927553\n[63]\ttraining's rmse: 0.893738\tvalid_1's rmse: 0.926362\n[64]\ttraining's rmse: 0.891471\tvalid_1's rmse: 0.925295\n[65]\ttraining's rmse: 0.889137\tvalid_1's rmse: 0.924112\n[66]\ttraining's rmse: 0.886598\tvalid_1's rmse: 0.923104\n[67]\ttraining's rmse: 0.884316\tvalid_1's rmse: 0.92186\n[68]\ttraining's rmse: 0.882099\tvalid_1's rmse: 0.920897\n[69]\ttraining's rmse: 0.879915\tvalid_1's rmse: 0.920129\n[70]\ttraining's rmse: 0.87787\tvalid_1's rmse: 0.919481\n[71]\ttraining's rmse: 0.87589\tvalid_1's rmse: 0.918517\n[72]\ttraining's rmse: 0.87388\tvalid_1's rmse: 0.917738\n[73]\ttraining's rmse: 0.871671\tvalid_1's rmse: 0.916982\n[74]\ttraining's rmse: 0.869665\tvalid_1's rmse: 0.916224\n[75]\ttraining's rmse: 0.867756\tvalid_1's rmse: 0.915379\n[76]\ttraining's rmse: 0.865879\tvalid_1's rmse: 0.914985\n[77]\ttraining's rmse: 0.863936\tvalid_1's rmse: 0.914616\n[78]\ttraining's rmse: 0.862082\tvalid_1's rmse: 0.913623\n[79]\ttraining's rmse: 0.860169\tvalid_1's rmse: 0.912979\n[80]\ttraining's rmse: 0.858335\tvalid_1's rmse: 0.912631\n[81]\ttraining's rmse: 0.856638\tvalid_1's rmse: 0.911784\n[82]\ttraining's rmse: 0.855098\tvalid_1's rmse: 0.91105\n[83]\ttraining's rmse: 0.853326\tvalid_1's rmse: 0.910542\n[84]\ttraining's rmse: 0.851833\tvalid_1's rmse: 0.90981\n[85]\ttraining's rmse: 0.850419\tvalid_1's rmse: 0.909288\n[86]\ttraining's rmse: 0.848751\tvalid_1's rmse: 0.908936\n[87]\ttraining's rmse: 0.847243\tvalid_1's rmse: 0.908282\n[88]\ttraining's rmse: 0.84568\tvalid_1's rmse: 0.907777\n[89]\ttraining's rmse: 0.844327\tvalid_1's rmse: 0.907253\n[90]\ttraining's rmse: 0.842934\tvalid_1's rmse: 0.906601\n[91]\ttraining's rmse: 0.84173\tvalid_1's rmse: 0.906076\n[92]\ttraining's rmse: 0.840482\tvalid_1's rmse: 0.905789\n[93]\ttraining's rmse: 0.839168\tvalid_1's rmse: 0.905475\n[94]\ttraining's rmse: 0.837388\tvalid_1's rmse: 0.90524\n[95]\ttraining's rmse: 0.836169\tvalid_1's rmse: 0.904775\n[96]\ttraining's rmse: 0.834893\tvalid_1's rmse: 0.904439\n[97]\ttraining's rmse: 0.833485\tvalid_1's rmse: 0.90407\n[98]\ttraining's rmse: 0.83213\tvalid_1's rmse: 0.903451\n[99]\ttraining's rmse: 0.830978\tvalid_1's rmse: 0.903257\n[100]\ttraining's rmse: 0.829683\tvalid_1's rmse: 0.903137\n[101]\ttraining's rmse: 0.828481\tvalid_1's rmse: 0.902734\n[102]\ttraining's rmse: 0.827338\tvalid_1's rmse: 0.902223\n[103]\ttraining's rmse: 0.826318\tvalid_1's rmse: 0.901733\n[104]\ttraining's rmse: 0.825093\tvalid_1's rmse: 0.901551\n[105]\ttraining's rmse: 0.823975\tvalid_1's rmse: 0.901435\n[106]\ttraining's rmse: 0.822875\tvalid_1's rmse: 0.901205\n[107]\ttraining's rmse: 0.821847\tvalid_1's rmse: 0.900988\n[108]\ttraining's rmse: 0.820844\tvalid_1's rmse: 0.900566\n[109]\ttraining's rmse: 0.819795\tvalid_1's rmse: 0.900307\n[110]\ttraining's rmse: 0.818965\tvalid_1's rmse: 0.900186\n[111]\ttraining's rmse: 0.817855\tvalid_1's rmse: 0.899943\n[112]\ttraining's rmse: 0.817091\tvalid_1's rmse: 0.899718\n[113]\ttraining's rmse: 0.815961\tvalid_1's rmse: 0.899629\n[114]\ttraining's rmse: 0.814868\tvalid_1's rmse: 0.899566\n[115]\ttraining's rmse: 0.814049\tvalid_1's rmse: 0.899299\n[116]\ttraining's rmse: 0.813205\tvalid_1's rmse: 0.899218\n[117]\ttraining's rmse: 0.81235\tvalid_1's rmse: 0.899025\n[118]\ttraining's rmse: 0.811604\tvalid_1's rmse: 0.898889\n[119]\ttraining's rmse: 0.810886\tvalid_1's rmse: 0.898751\n[120]\ttraining's rmse: 0.809999\tvalid_1's rmse: 0.898648\n[121]\ttraining's rmse: 0.809166\tvalid_1's rmse: 0.898398\n[122]\ttraining's rmse: 0.808242\tvalid_1's rmse: 0.898259\n[123]\ttraining's rmse: 0.807541\tvalid_1's rmse: 0.898314\n[124]\ttraining's rmse: 0.806747\tvalid_1's rmse: 0.897951\n[125]\ttraining's rmse: 0.805875\tvalid_1's rmse: 0.897912\n[126]\ttraining's rmse: 0.804846\tvalid_1's rmse: 0.897944\n[127]\ttraining's rmse: 0.804148\tvalid_1's rmse: 0.897823\n[128]\ttraining's rmse: 0.803295\tvalid_1's rmse: 0.897603\n[129]\ttraining's rmse: 0.802442\tvalid_1's rmse: 0.897352\n[130]\ttraining's rmse: 0.801742\tvalid_1's rmse: 0.897211\n[131]\ttraining's rmse: 0.800946\tvalid_1's rmse: 0.897235\n[132]\ttraining's rmse: 0.800033\tvalid_1's rmse: 0.897094\n[133]\ttraining's rmse: 0.799136\tvalid_1's rmse: 0.896951\n[134]\ttraining's rmse: 0.798384\tvalid_1's rmse: 0.897304\n[135]\ttraining's rmse: 0.79771\tvalid_1's rmse: 0.897131\n[136]\ttraining's rmse: 0.797041\tvalid_1's rmse: 0.896808\n[137]\ttraining's rmse: 0.796248\tvalid_1's rmse: 0.896619\n[138]\ttraining's rmse: 0.795579\tvalid_1's rmse: 0.896517\n[139]\ttraining's rmse: 0.794891\tvalid_1's rmse: 0.896235\n[140]\ttraining's rmse: 0.794199\tvalid_1's rmse: 0.896262\n[141]\ttraining's rmse: 0.793523\tvalid_1's rmse: 0.896351\n[142]\ttraining's rmse: 0.792382\tvalid_1's rmse: 0.896764\n[143]\ttraining's rmse: 0.791638\tvalid_1's rmse: 0.89687\n[144]\ttraining's rmse: 0.790937\tvalid_1's rmse: 0.896913\n[145]\ttraining's rmse: 0.790237\tvalid_1's rmse: 0.897194\n[146]\ttraining's rmse: 0.78967\tvalid_1's rmse: 0.897196\n[147]\ttraining's rmse: 0.789098\tvalid_1's rmse: 0.897002\n","name":"stdout"},{"output_type":"stream","text":"[148]\ttraining's rmse: 0.788386\tvalid_1's rmse: 0.896957\n[149]\ttraining's rmse: 0.78762\tvalid_1's rmse: 0.897298\n[150]\ttraining's rmse: 0.787146\tvalid_1's rmse: 0.897248\n[151]\ttraining's rmse: 0.786546\tvalid_1's rmse: 0.897149\n[152]\ttraining's rmse: 0.786042\tvalid_1's rmse: 0.897321\n[153]\ttraining's rmse: 0.785446\tvalid_1's rmse: 0.897185\n[154]\ttraining's rmse: 0.784977\tvalid_1's rmse: 0.897199\n[155]\ttraining's rmse: 0.784495\tvalid_1's rmse: 0.897194\n[156]\ttraining's rmse: 0.783671\tvalid_1's rmse: 0.897028\n[157]\ttraining's rmse: 0.783177\tvalid_1's rmse: 0.896969\n[158]\ttraining's rmse: 0.782558\tvalid_1's rmse: 0.896935\n[159]\ttraining's rmse: 0.781825\tvalid_1's rmse: 0.896966\nEarly stopping, best iteration is:\n[139]\ttraining's rmse: 0.794891\tvalid_1's rmse: 0.896235\n","name":"stdout"},{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"0.8962350082076307"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["Y_pred = model.predict(X_valid).clip(0, 20)\n","Y_test = model.predict(X_test).clip(0, 20)\n","\n","submission = pd.DataFrame({\n","    \"ID\": test.index, \n","    \"item_cnt_month\": Y_test\n","})\n","submission.to_csv('submission.csv', index=False)\n","\n","# save predictions for an ensemble\n","pickle.dump(Y_pred, open('lgb_train.pickle', 'wb'))\n","pickle.dump(Y_test, open('lgb_test.pickle', 'wb'))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# from lightgbm import plot_importance\n","# from xgboost import plot_importance\n","from lightgbm import plot_importance\n","\n","def plot_features(booster, figsize):    \n","    fig, ax = plt.subplots(1,1,figsize=figsize)\n","    return plot_importance(booster=booster, ax=ax)\n","\n","plot_features(model, (10,14))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["feat_importance = model.feature_importance()\n","df_importance = pd.DataFrame(feat_importance, columns=['importance'], index=X_test.columns)\n","df_importance = df_importance.sort_values(by='importance', ascending=False)\n","df_importance.index\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_importance"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}